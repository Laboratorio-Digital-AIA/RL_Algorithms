{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mountain_Car_Threads.ipynb","provenance":[],"authorship_tag":"ABX9TyNl0h2/DZnkhWsKB4XePr5+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"PYw9qgy9K7yk"},"source":["!pip install gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nE3qndGqKdQ5","executionInfo":{"status":"ok","timestamp":1634187597822,"user_tz":300,"elapsed":1296,"user":{"displayName":"Santiago Bobadilla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04195604726504183829"}},"outputId":"8b427a82-68e2-4950-c586-3bd3aa56cbe0"},"source":["\n","# --- SANTIAGO BOBADILLA\n","# --- 13/10/2021\n","# ___________________________\n","\n","\n","\n","# --- Libraries.\n","# ______________\n","\n","import logging\n","import threading\n","\n","import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","\n","# --- Thread Log.\n","# _______________\n","\n","format = \"%(asctime)s: %(message)s\"\n","logging.basicConfig(format = format, level = logging.INFO, datefmt=\"%H:%M:%S\")\n","\n","\n","\n","# --- Q Learning Multi Thread.\n","# ______________________________\n","\n","class QL (threading.Thread):\n","\n","\n","\n","    # --- Static variables of the monitor.\n","    # ____________________________________\n","\n","    # .. Lock for concurrency and counter for tests.\n","\n","    _counter = 0\n","    _lock = threading.Lock()\n","\n","    # .. Q Table and common parameters.\n","    #       .. Each thread has his environment, but we created an initial reference for the dimensions of the Q Table.\n","\n","    _ref_inicial = gym.make(\"MountainCar-v0\")\n","    _ref_inicial.reset()\n","\n","    DESCRETE_OS_SIZE = [40] * len(_ref_inicial.observation_space.high)\n","    DESCRETE_OS_WIN_SIZE  = (_ref_inicial.observation_space.high - _ref_inicial.observation_space.low) / DESCRETE_OS_SIZE\n","\n","    q_table = np.random.uniform(low = -2, high = 0, size = ( DESCRETE_OS_SIZE  + [_ref_inicial.action_space.n] ))\n","\n","    LEARNING_RATE = 0.1 \n","    DISCOUNT = 0.95\n","\n","\n","\n","    # --- Constructor of each thread\n","    # _______________________________\n","\n","    def __init__ (self, id_thread: int):\n","\n","        # .. ID's\n","\n","        super().__init__()\n","        self.id_thread = id_thread\n","\n","        # .. Environment\n","\n","        self.env =gym.make(\"MountainCar-v0\")\n","        self.env.reset()\n","\n","        # .. Algorithm parameters\n","\n","        self.EPISODES = 1000\n","        self.SHOW_EVERY = 100\n","\n","        self.epsilon = 0.5\n","        self.START_EPSILON_DECAY = 1\n","        self.END_EPSILON_DECAY = self.EPISODES // 2\n","        self.epsilon_decay_value = self.epsilon / (self.END_EPSILON_DECAY - self.START_EPSILON_DECAY)\n","\n","        # .. Result parameters\n","\n","        self.eps_reward = []\n","        self.aggr_ep_reward = { 'eps' : [], 'avg' : [], 'min' : [], 'max' : [] }\n","\n","        # .. Local own information\n","\n","        self.discrete_state = 0\n","        self.episode_reward = 0\n","        self.discrete_state = 0\n","        self.done = False\n","        self.action = 0\n","        self.new_state = 0\n","        self.reward = 0\n","        self.new_discrete_state = 0\n","        self._state = 0\n","        self.average_reward = 0\n","\n","\n","\n","    # --- Helper function to fit values to the Q ~ Table\n","    # ___________________________________________________\n","\n","    def get_descrete_state(self, state):\n","        self._state = (state - self.env.observation_space.low) / self.DESCRETE_OS_WIN_SIZE\n","        return tuple(self._state.astype(np.int))\n","\n","\n","    # --- RUN Q Learning\n","    # ___________________\n","\n","    def Q_Learning(self):\n","\n","        for episode in range(self.EPISODES):\n","\n","            self.episode_reward = 0\n","\n","            self.discrete_state = self.get_descrete_state(self.env.reset())\n","            self.done = False\n","\n","            while not self.done:\n","\n","                if np.random.random() > self.epsilon:\n","\n","                    # .. Concurrency lock\n","                    ## -->\n","                    QL._lock.acquire()\n","\n","                    self.action = np.argmax(QL.q_table[self.discrete_state])\n","\n","                    QL._lock.release()\n","                    # # <---\n","\n","                else:                                                                                               \n","                    self.action = np.random.randint(0, self.env.action_space.n)\n","\n","                self.new_state, self.reward, self.done, _ = self.env.step(self.action)\n","                self.episode_reward += self.reward\n","                self.new_discrete_state = self.get_descrete_state(self.new_state)\n","\n","                # .. Concurrency lock\n","                # -->\n","                QL._lock.acquire()\n","\n","                if not self.done:\n","\n","                    new_q = (1-QL.LEARNING_RATE)* QL.q_table[self.discrete_state + (self.action,)] + QL.LEARNING_RATE * (self.reward + (np.max(QL.q_table[self.new_discrete_state]) * QL.DISCOUNT))\n","                    QL.q_table[self.discrete_state + (self.action,)] = new_q\n","\n","                elif self.new_state[0] >= self.env.goal_position: \n","\n","                    print(f\"We made it to the flag on episode {episode}\")\n","                    QL.q_table[self.discrete_state + (self.action,)] = 0 \n","\n","                QL._lock.release()\n","                # <---\n","\n","                self.discrete_state = self.new_discrete_state\n","\n","            if self.END_EPSILON_DECAY >= episode >= self.START_EPSILON_DECAY:\n","                self.epsilon -= self.epsilon_decay_value\n","\n","            self.eps_reward.append(self.episode_reward)\n","\n","            if not episode % self.SHOW_EVERY:\n","\n","                average_reward = sum(self.eps_reward[-self.SHOW_EVERY:])/len(self.eps_reward[-self.SHOW_EVERY:])\n","\n","                self.aggr_ep_reward['eps'].append(episode)\n","                self.aggr_ep_reward['avg'].append(average_reward)\n","                self.aggr_ep_reward['min'].append(min(self.eps_reward[-self.SHOW_EVERY:]))\n","                self.aggr_ep_reward['max'].append(max(self.eps_reward[-self.SHOW_EVERY:]))\n","\n","                print(f\"Thread {self.id_thread} --> Episode: {episode} Avg: {average_reward} Min: {min(self.eps_reward[-self.SHOW_EVERY:])} Max: {max(self.eps_reward[-self.SHOW_EVERY:])}\" )\n","\n","        self.env.close()\n","\n","    # --- RUN \n","    # _________\n","\n","    def run(self):\n","        self.Q_Learning()\n","\n","\n","# --- MAIN\n","# _________\n","\n","NUM_THREADS = 5\n","\n","for i in range(NUM_THREADS):\n","    thread = QL(i)\n","    thread.start()\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Thread 0 --> Episode: 0 Avg: -200.0 Min: -200.0 Max: -200.0\n","Thread 2 --> Episode: 0 Avg: -200.0 Min: -200.0 Max: -200.0\n","Thread 1 --> Episode: 0 Avg: -200.0 Min: -200.0 Max: -200.0\n"]}]}]}